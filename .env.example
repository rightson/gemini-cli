# =================================================================
# Gemini CLI Model Configuration
# =================================================================
# Override default Gemini models with custom models (OpenAI, local, etc.)
# Uncomment and modify the values you want to override

# =================================================================
# MODEL OVERRIDES - All Available Keys
# =================================================================

# --- Primary model (used for main content generation) ---
# Default: gemini-2.5-pro
DEFAULT_GEMINI_MODEL=gpt-4.1

# --- Flash model (used for fallback, error recovery, faster operations) ---
# Default: gemini-2.5-flash  
DEFAULT_GEMINI_FLASH_MODEL=gpt-4.1-mini

# --- Flash lite model (used for lightweight operations: edits, summaries, corrections) ---
# Default: gemini-2.5-flash-lite
DEFAULT_GEMINI_FLASH_LITE_MODEL=gpt-4.1-mini

# --- Embedding model (used for text embeddings, semantic search) ---
# Default: gemini-embedding-001
DEFAULT_GEMINI_EMBEDDING_MODEL=text-embedding-ada-002

# =================================================================
# MINIMAL CONFIGURATION (2 models only)
# =================================================================
# If you only want to override the most important models, use these:
# DEFAULT_GEMINI_MODEL=gpt-4.1                    # Primary model
# DEFAULT_GEMINI_FLASH_LITE_MODEL=gpt-4.1-mini    # Lightweight operations

# =================================================================
# CUSTOM TOKEN LIMITS
# =================================================================
# Format: MODEL_TOKEN_LIMIT_<MODEL_NAME_UPPERCASE>=<LIMIT>
# Convert model names: gpt-4.1 → GPT_4_1, custom-model → CUSTOM_MODEL

# OpenAI models
MODEL_TOKEN_LIMIT_GPT_4=128000
MODEL_TOKEN_LIMIT_GPT_4_1=128000
MODEL_TOKEN_LIMIT_GPT_4_TURBO=128000
MODEL_TOKEN_LIMIT_GPT_3_5_TURBO=16384
MODEL_TOKEN_LIMIT_GPT_4_1_MINI=16384

# Local models (e.g., Ollama)
# MODEL_TOKEN_LIMIT_LLAMA3_1_70B=32768
# MODEL_TOKEN_LIMIT_LLAMA3_1_8B=8192
# MODEL_TOKEN_LIMIT_CODELLAMA_34B=16384

# Claude models
# MODEL_TOKEN_LIMIT_CLAUDE_3_OPUS=200000
# MODEL_TOKEN_LIMIT_CLAUDE_3_SONNET=200000
# MODEL_TOKEN_LIMIT_CLAUDE_3_HAIKU=200000

# Custom/Company models
# MODEL_TOKEN_LIMIT_COMPANY_MODEL_V2=64000
# MODEL_TOKEN_LIMIT_COMPANY_MODEL_LITE=16000

# =================================================================
# API CONFIGURATION
# =================================================================

# --- OpenAI API ---
OPENAI_API_KEY=your-openai-api-key-here
# OPENAI_BASE_URL=https://api.openai.com/v1

# --- OpenAI-Compatible Endpoints (Local/Custom) ---
# For Ollama:
# OPENAI_API_KEY=ollama
# OPENAI_BASE_URL=http://localhost:11434/v1

# For local server:
# OPENAI_API_KEY=your-local-api-key
# OPENAI_BASE_URL=http://localhost:8000/v1

# For company proxy:
# OPENAI_API_KEY=your-company-key
# OPENAI_BASE_URL=https://company-ai-proxy.internal/v1

# =================================================================
# EXAMPLE CONFIGURATIONS
# =================================================================

# --- Example 1: Full OpenAI Replacement ---
# DEFAULT_GEMINI_MODEL=gpt-4
# DEFAULT_GEMINI_FLASH_MODEL=gpt-3.5-turbo
# DEFAULT_GEMINI_FLASH_LITE_MODEL=gpt-3.5-turbo
# DEFAULT_GEMINI_EMBEDDING_MODEL=text-embedding-ada-002

# --- Example 2: Local Ollama Models ---
# DEFAULT_GEMINI_MODEL=llama3.1:70b
# DEFAULT_GEMINI_FLASH_MODEL=llama3.1:8b
# DEFAULT_GEMINI_FLASH_LITE_MODEL=llama3.1:8b
# OPENAI_BASE_URL=http://localhost:11434/v1

# --- Example 3: Mixed Setup (OpenAI + Local) ---
# DEFAULT_GEMINI_MODEL=gpt-4                      # OpenAI for quality
# DEFAULT_GEMINI_FLASH_MODEL=llama3.1:8b          # Local for speed
# DEFAULT_GEMINI_FLASH_LITE_MODEL=llama3.1:8b     # Local for efficiency

# --- Example 4: Company In-House Models ---
# DEFAULT_GEMINI_MODEL=company-model-v2
# DEFAULT_GEMINI_FLASH_MODEL=company-model-fast
# DEFAULT_GEMINI_FLASH_LITE_MODEL=company-model-lite
# OPENAI_BASE_URL=https://company-ai-api.internal/v1